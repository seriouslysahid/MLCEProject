{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ce7c69",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Techniques: Histograms, PDF overlay, KDE, boxplots, correlation heatmaps, pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74134e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset\n",
    "data = r'C:\\Users\\Student\\Downloads\\HTL_data.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first five entries of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the statistics about the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2051c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain information about the variable types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "features = ['bwr', 'temp', 'time', 'pressure']\n",
    "\n",
    "# Ensure that none of the columns should be missed to be read properly\n",
    "missing = [col for col in features if col not in df.columns]\n",
    "print(\"Missing columns:\", missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a219e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms\n",
    "sns.histplot(df.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c076826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram plot along with the PDF and KDE\n",
    "x = np.linspace(min(df.temp), max(df.temp), 1000)\n",
    "mu = np.mean(df.temp)\n",
    "sigma = np.std(df.temp)\n",
    "\n",
    "y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "plt.plot(x, y, color=\"red\", label=\"Gaussian PDF\")\n",
    "sns.histplot(df.temp, kde=True, stat=\"density\", color=\"blue\", label=\"Histogram+KDE\")\n",
    "\n",
    "# Computing the bandwidth using Scott's Rule\n",
    "n = len(df.temp)\n",
    "h_scott = n**(-1/5) * sigma\n",
    "sns.kdeplot(df.temp, bw_method=h_scott*0.03, color=\"green\", label=\"KDE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density plots for all features\n",
    "df[features].plot(kind=\"density\", subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7106f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate boxplot with outlier detection\n",
    "sns.boxplot(df.temp)\n",
    "\n",
    "# Manual Evaluation Of the boxplot\n",
    "Q1 = np.percentile(df.temp, 25)\n",
    "Q3 = np.percentile(df.temp, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Defining the lb and ub\n",
    "lb = Q1 - 1.5*IQR\n",
    "ub = Q3 + 1.5*IQR\n",
    "outlier = df.temp[(df.temp < lb) | (df.temp > ub)]\n",
    "print(outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate boxplot\n",
    "sns.boxplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "corr_mat = df.corr()\n",
    "display(corr_mat)\n",
    "sns.heatmap(corr_mat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual correlation computation\n",
    "cov_TP = np.cov(df.temp, df.pressure)\n",
    "print(cov_TP)\n",
    "\n",
    "corr_m = cov_TP / (np.std(df.temp) * np.std(df.pressure))\n",
    "print(corr_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9206794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.scatter(df.bwr, df.temp)\n",
    "plt.xlabel(\"bwr\")\n",
    "plt.ylabel(\"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplots\n",
    "sns.pairplot(df)\n",
    "# sns.pairplot(df[['temp', 'time']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93258b42",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Linear Regression (Least Squares)\n",
    "\n",
    "Manual implementation with design matrix, parity plot, confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b845408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e574bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r\"C:\\Users\\Student\\Downloads\\Example_ML.csv\"\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb201d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((df.FA, df.R, df.Age))\n",
    "n = len(df.FA)\n",
    "\n",
    "# Design matrix\n",
    "X_d = np.column_stack((np.ones(n), X))\n",
    "y = df.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Least squares solution\n",
    "beta_hat = np.linalg.inv(X_d.T @ X_d) @ X_d.T @ y\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20ee060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted value of house price\n",
    "y_hat = X_d @ beta_hat\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals and metrics\n",
    "residual = y - y_hat\n",
    "SSR = np.sum(residual**2)\n",
    "\n",
    "# Sum of squared residuals\n",
    "SST = np.sum((y - np.mean(y))**2)\n",
    "\n",
    "R2 = 1 - (SSR / SST)\n",
    "print(R2)\n",
    "\n",
    "# Mean Absolute Error\n",
    "MAE = np.mean(abs(residual))\n",
    "print(MAE)\n",
    "\n",
    "# Mean Squared Error\n",
    "MSE = SSR / n\n",
    "print(MSE)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b99697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity Plot\n",
    "plt.scatter(y, y_hat)\n",
    "plt.plot([y.min(), y.max()], [y_hat.min(), y_hat.max()], 'k--')\n",
    "plt.xlabel(\"y_observed\")\n",
    "plt.ylabel(\"y_predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of parameters to be estimated\n",
    "p = 4\n",
    "\n",
    "# sigma_hat2\n",
    "sigma2 = SSR / (n - p)\n",
    "\n",
    "# Covariance matrix of beta\n",
    "cov_beta = sigma2 * np.linalg.inv(X_d.T @ X_d)\n",
    "print(cov_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a70204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard errors in estimated beta_hat\n",
    "se_beta = np.sqrt(np.diag(cov_beta))\n",
    "print(se_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49303350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical t-score\n",
    "alpha = 0.05\n",
    "t_score = stats.t.ppf(1 - alpha/2, n - p)\n",
    "print(t_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f37a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the confidence intervals\n",
    "lb = beta_hat - t_score * se_beta\n",
    "ub = beta_hat + t_score * se_beta\n",
    "\n",
    "CI = np.column_stack((lb, beta_hat, ub))\n",
    "print(CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a7901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(FA, R, Age):\n",
    "    X_d = np.array([1, FA, R, Age])\n",
    "    y_pred = X_d @ beta_hat\n",
    "    return y_pred\n",
    "\n",
    "y_new = predict(FA=200, R=3, Age=5)\n",
    "print(y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7432c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Ridge Regression (Regularization)\n",
    "\n",
    "L2 regularization with penalty parameter lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f668aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of observations\n",
    "n = 100\n",
    "\n",
    "X = np.linspace(0, 5, n)\n",
    "\n",
    "# Observed data points\n",
    "y_true = 3 + 2*X - 0.5*X**2 + 0.1*X**3 - 0.05*X**4\n",
    "\n",
    "# Observations corrupted with noise (0 Mean and 0.5 Variance)\n",
    "y = y_true + np.random.normal(0, 0.5, size=n)\n",
    "\n",
    "X_features = np.column_stack([X, X**2, X**3, X**4])\n",
    "\n",
    "X_train, X_Test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "\n",
    "mean_train = X_train.mean(axis=0)\n",
    "std_train = X_train.std(axis=0, ddof=0)\n",
    "\n",
    "X_train_scaled = (X_train - mean_train) / std_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing/scaling the input features with 0 mean and unit standard deviation\n",
    "X_scaled = np.column_stack([X_train_scaled, X_train_scaled**2, X_train_scaled**3, X_train_scaled**4])\n",
    "\n",
    "# Design matrix for the scaled training inputs\n",
    "X_d = np.column_stack([np.ones(len(X_scaled)), X_scaled])\n",
    "\n",
    "# Identity matrix comprising of slope only\n",
    "I = np.eye(X_d.shape[1])\n",
    "print(I)\n",
    "\n",
    "# Penalise the slopes only not the intercept\n",
    "I[0, 0] = 0\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27107dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalty parameter\n",
    "lamda = 1\n",
    "\n",
    "# Least squares estimates for Ridge Regression\n",
    "beta_ridge = np.linalg.inv(X_d.T @ X_d + lamda*I) @ X_d.T @ y_train\n",
    "print(beta_ridge)\n",
    "\n",
    "# Predict y based on the scaled X_train\n",
    "y_pred_train = X_d @ beta_ridge\n",
    "\n",
    "plt.scatter(y_pred_train, y_train)\n",
    "plt.plot([y_pred_train.min(), y_pred_train.max()], [y_pred_train.min(), y_pred_train.max()], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225dc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals based on the scaled training inputs\n",
    "residual_train = (y_pred_train - y_train)\n",
    "\n",
    "SSR_train = np.sum(residual_train**2)\n",
    "SST_train = np.sum((y_train - np.mean(y_train))**2)\n",
    "\n",
    "R2_train = 1 - (SSR_train / SST_train)\n",
    "print(f\"R2_train: {R2_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537b107",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "MLE for Normal and Bernoulli distributions using optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45289053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b892dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some observed data (Normal distribution with mu=5, sigma=2)\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(5, 2, size=100)\n",
    "\n",
    "# Negative log-likelihood function for Normal distribution\n",
    "def neg_log_likelihood(params, data):\n",
    "    mu, sigma = params[0], params[1]\n",
    "    if sigma <= 0:  # variance must be positive\n",
    "        return np.inf\n",
    "    n = len(data)\n",
    "    return 0.5 * n * np.log(2 * np.pi * sigma**2) + np.sum((data - mu)**2) / (2 * sigma**2)\n",
    "\n",
    "# Initial guess (mean of data, std of data)\n",
    "init_params = [10, 5]\n",
    "\n",
    "# Minimize negative log-likelihood\n",
    "result = minimize(neg_log_likelihood, init_params, args=(data,), method='nelder-mead', bounds=[(None, None), (0, None)])\n",
    "mu_mle, sigma_mle = result.x\n",
    "\n",
    "print(\"MLE mean (mu):\", mu_mle)\n",
    "print(\"MLE variance (sigma^2):\", sigma_mle**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d65e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some Bernoulli data (true p=0.7)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 10 independent coin toss experiments\n",
    "data = np.random.binomial(n=1, p=0.7, size=10)\n",
    "\n",
    "# Negative log-likelihood for Bernoulli\n",
    "def neg_log_likelihood(p, data):\n",
    "    if p <= 0 or p >= 1:  # ensure p in (0,1)\n",
    "        return np.inf\n",
    "    ll = -np.sum(data * np.log(p) + (1 - data) * np.log(1 - p))\n",
    "    return ll\n",
    "\n",
    "# Initial guess of p\n",
    "init_params = [0.3]\n",
    "\n",
    "# Optimize p using an optimization solver\n",
    "res = minimize(neg_log_likelihood, init_params, args=(data,), method='nelder-mead', bounds=[(0, 1)])\n",
    "\n",
    "p_mle = res.x\n",
    "\n",
    "print(\"MLE of p (via optimization):\", p_mle)\n",
    "print(\"Closed-form MLE of p (sample mean):\", np.mean(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40363f08",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Principal Component Analysis (PCA)\n",
    "\n",
    "Manual PCA implementation with eigendecomposition and scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r'C:\\Users\\Riju\\Desktop\\HTL_data.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55233e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "display(cov_matrix)\n",
    "\n",
    "# Eigen decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues (and eigenvectors) in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Compute explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / np.sum(eigenvalues)\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display eigenvalues and explained variance\n",
    "print(\"\\nEigenvalues (Descending):\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nExplained Variance Ratio (%):\")\n",
    "for i, var in enumerate(explained_variance_ratio):\n",
    "    print(f\"PC{i+1}: {var*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nCumulative Variance Explained: {cumulative_variance*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto principal components\n",
    "Z = X_scaled @ eigenvectors  # principal component scores\n",
    "\n",
    "# Choose number of components\n",
    "n_components = 3\n",
    "Z_reduced = Z[:, :n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8020c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot explained variance (Scree plot)\n",
    "plt.figure(figsize=(6, 4))\n",
    "PC_No = [1, 2, 3, 4]\n",
    "plt.plot(PC_No, eigenvalues)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.title('Scree Plot')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae093e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Support Vector Regression (SVR)\n",
    "\n",
    "SVR with RBF kernel, GridSearchCV for hyperparameter tuning, cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = r'C:\\Users\\Riju\\Desktop\\HTL_data.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b989f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR())\n",
    "])\n",
    "\n",
    "# L2 regularization\n",
    "# Smaller C = stronger regularization\n",
    "param_grid = {\n",
    "    \"svr__kernel\": [\"rbf\"],\n",
    "    \"svr__C\": [1, 5, 10, 20, 50, 100],\n",
    "    \"svr__epsilon\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"svr__gamma\": [\"scale\", \"auto\", 0.1, 0.01]\n",
    "}\n",
    "\n",
    "# 5-fold CV\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    return_train_score=True\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean CV Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07daa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Metrics function\n",
    "def metrics(true, pred, label):\n",
    "    print(f\"\\n{label} Metrics:\")\n",
    "    print(\"  RMSE:\", np.sqrt(mean_squared_error(true, pred)))\n",
    "    print(\"  MAE :\", mean_absolute_error(true, pred))\n",
    "    print(\"  R²  :\", r2_score(true, pred))\n",
    "\n",
    "metrics(y_train, y_train_pred, \"Training\")\n",
    "metrics(y_test, y_test_pred, \"Testing\")\n",
    "\n",
    "# Cross-validated predictions\n",
    "cv_pred = cross_val_predict(best_model, X, y, cv=cv)\n",
    "\n",
    "print(\"\\nCV Metrics:\")\n",
    "print(\"  RMSE:\", np.sqrt(mean_squared_error(y, cv_pred)))\n",
    "print(\"  MAE :\", mean_absolute_error(y, cv_pred))\n",
    "print(\"  R²  :\", r2_score(y, cv_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3981495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.title(\"Actual vs Predicted (Test)\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_train, y_train_pred, alpha=0.5)\n",
    "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')\n",
    "plt.title(\"Actual vs Predicted (Train)\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa892a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Support Vector Classification (SVM)\n",
    "\n",
    "SVM for fault classification with confusion matrix and decision boundary visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = r\"E:\\MLCourse\\MLxChE-main\\Week-04-Kernel-Methods\\evap_data.csv\"\n",
    "df = pd.read_csv(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943b6ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fault data\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "class_0 = df.index[df['Status'] == 'Normal']\n",
    "class_1 = df.index[df['Status'] == 'Pump_failure']\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(class_0, df.iloc[class_0][\"L2\"], 'b')\n",
    "plt.plot(class_1, df.iloc[class_1][\"L2\"], 'r')\n",
    "plt.grid()\n",
    "plt.ylabel('L2')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(class_0, df.iloc[class_0][\"P2\"], 'b')\n",
    "plt.plot(class_1, df.iloc[class_1][\"P2\"], 'r')\n",
    "plt.grid()\n",
    "plt.ylabel('P2')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(class_0, df.iloc[class_0][\"X2\"], 'b', label='Normal Operation')\n",
    "plt.plot(class_1, df.iloc[class_1][\"X2\"], 'r', label='Pump Failure')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('X2')\n",
    "plt.xlabel('Sample number')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :3].to_numpy()\n",
    "y = LabelEncoder().fit_transform(df.iloc[:, -1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data set\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "N_train = len(X_train_scaled)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(X_train_scaled[y_train == 0, 0],\n",
    "           X_train_scaled[y_train == 0, 1],\n",
    "           X_train_scaled[y_train == 0, 2],\n",
    "           c='b', label='Normal operation')\n",
    "\n",
    "ax.scatter(X_train_scaled[y_train == 1, 0],\n",
    "           X_train_scaled[y_train == 1, 1],\n",
    "           X_train_scaled[y_train == 1, 2],\n",
    "           c='r', label='Pump failure')\n",
    "ax.set_xlabel('Scaled L2')\n",
    "ax.set_ylabel('Scaled P2')\n",
    "ax.set_zlabel('Scaled X2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce4a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM with different hyperparameters\n",
    "box_C = np.array([1, 1, 1, 10, 10, 10, 100, 100, 100])\n",
    "gamma = np.array([0.1, 1, 2, 0.1, 1, 2, 0.1, 1, 2])\n",
    "\n",
    "for i in range(len(box_C)):\n",
    "    print(f\"Box Constraint: {box_C[i]}, gamma = {gamma[i]}\")\n",
    "    mdl = SVC(C=box_C[i], gamma=gamma[i]).fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred_train = mdl.predict(X_train_scaled)\n",
    "    y_pred_test = mdl.predict(X_test_scaled)\n",
    "\n",
    "    print(f\"  Accuracy for training: {mdl.score(X_train_scaled, y_train)}\")\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    print(cm_train)\n",
    "\n",
    "    print(f\"  Accuracy for testing: {mdl.score(X_test_scaled, y_test)}\")\n",
    "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "    print(cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb9d75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using L2 and P2 only (for illustration purposes)\n",
    "X = df.iloc[:, :2].to_numpy()\n",
    "y = LabelEncoder().fit_transform(df.iloc[:, -1])\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_scaled = sc.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='b', label='Normal Operation')\n",
    "plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='r', label='Pump failure')\n",
    "plt.xlabel('Scaled L2')\n",
    "plt.ylabel('Scaled P2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa3b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision boundary visualization\n",
    "mdl = SVC().fit(X_scaled, y)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "DecisionBoundaryDisplay.from_estimator(mdl, X_scaled, cmap='bwr', alpha=0.8)\n",
    "plt.scatter(X_scaled[y == 0, 0], X_scaled[y == 0, 1], c='b', edgecolors=\"k\")\n",
    "plt.scatter(X_scaled[y == 1, 0], X_scaled[y == 1, 1], c='r', edgecolors=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f02d9",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Logistic Regression (Binary Classification)\n",
    "\n",
    "1D and 2D logistic regression with sigmoid curve and decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cf6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Logistic Regression Example\n",
    "X_train = [1.2, 3.1, 4, 3.8, 2.5, 7, 5.2, 5.5, 6.1, 7.5]\n",
    "y_train = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "Xs = np.array(X_train).reshape(-1, 1)\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(Xs, y_train)\n",
    "\n",
    "# Generate range of feature values for plotting sigmoid\n",
    "x_vals = np.linspace(min(Xs)-1, max(Xs)+1, 100).reshape(-1, 1)\n",
    "y_probs = clf.predict_proba(x_vals)[:, 1]\n",
    "\n",
    "# Find the decision boundary (where sigma(x)=0.5)\n",
    "boundary = -clf.intercept_[0] / clf.coef_[0]\n",
    "print(boundary)\n",
    "\n",
    "# Plot data and sigmoid curve\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(Xs, y_train, c=y_train, cmap=plt.cm.bwr, edgecolors='k', s=80)\n",
    "plt.plot(x_vals, y_probs, color=\"black\", linewidth=2, label=\"Sigmoid curve\")\n",
    "\n",
    "# Add vertical line for sigma(x) = 0.5\n",
    "plt.axvline(boundary, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Decision boundary (σ=0.5)\")\n",
    "\n",
    "plt.title(\"1D Logistic Regression with Decision Boundary\")\n",
    "plt.xlabel(\"Feature value\")\n",
    "plt.ylabel(\"Probability of Class 1\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Decision boundary (x where σ=0.5):\", boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Logistic Regression Example\n",
    "n_samples = 200\n",
    "\n",
    "x1 = np.random.randn(n_samples)\n",
    "x2 = np.random.randn(n_samples)\n",
    "\n",
    "# True boundary: y = 1 if x1 + x2 > 0 else 0\n",
    "y = (x1 + x2 > 0).astype(int)\n",
    "X = np.column_stack((x1, x2))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Call Logistic Regression for Binary Classification\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Intercept (β0):\", clf.intercept_[0])\n",
    "print(\"Coefficients (β1, β2):\", clf.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the sigmoid function for the given z data\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Take one test point\n",
    "x_test_point = X_test[0]   # first test sample\n",
    "print(\"\\nTest point (x1, x2):\", x_test_point)\n",
    "\n",
    "# Linear regression model\n",
    "z = clf.intercept_[0] + np.dot(clf.coef_[0], x_test_point)\n",
    "\n",
    "# Predicted probability using sigmoid\n",
    "y_hat_manual = sigmoid(z)\n",
    "print(\"Predicted probability (manual sigmoid):\", y_hat_manual)\n",
    "\n",
    "# Predicted probability using sklearn\n",
    "y_hat_sklearn = clf.predict_proba([x_test_point])[0, 1]\n",
    "print(\"Predicted probability (sklearn):\", y_hat_sklearn)\n",
    "\n",
    "# Predicted class (threshold 0.5)\n",
    "y_class = clf.predict([x_test_point])[0]\n",
    "print(\"Predicted class:\", y_class)\n",
    "print(\"Actual class:\", y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ae381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the decision boundary\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=\"bwr\", edgecolors=\"k\", s=60, label=\"Actual\")\n",
    "\n",
    "# Decision boundary grid\n",
    "x1_min, x1_max = X[:, 0].min()-1, X[:, 0].max()+1\n",
    "x2_min, x2_max = X[:, 1].min()-1, X[:, 1].max()+1\n",
    "x1x1, x2x2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n",
    "                         np.linspace(x2_min, x2_max, 100))\n",
    "\n",
    "Z = clf.predict(np.c_[x1x1.ravel(), x2x2.ravel()])\n",
    "Z = Z.reshape(x1x1.shape)\n",
    "\n",
    "plt.contourf(x1x1, x2x2, Z, alpha=0.3, cmap=\"bwr\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Logistic Regression Decision Boundary\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb57892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e878f",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Locally Weighted Regression\n",
    "\n",
    "Non-parametric regression using Gaussian kernel weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135913e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(30)\n",
    "n = 100\n",
    "\n",
    "X = np.linspace(-2*np.pi, 2*np.pi, n)\n",
    "y_true = np.sin(X)\n",
    "y = y_true + np.random.normal(0, 0.2, size=n)\n",
    "\n",
    "# Locally Weighted Regression\n",
    "tau = 0.6  # bandwidth\n",
    "X_design = np.column_stack((np.ones(n), X))   # design matrix with intercept\n",
    "\n",
    "X_test = np.linspace(-2*np.pi, 2*np.pi, 100)\n",
    "y_pred = []\n",
    "\n",
    "for x in X_test:\n",
    "    # Gaussian kernel weights\n",
    "    w = np.exp(- (X - x) ** 2 / (2 * tau ** 2))\n",
    "    W = np.diag(w)\n",
    "\n",
    "    # Weighted least squares estimates\n",
    "    theta = np.linalg.pinv(X_design.T @ W @ X_design) @ (X_design.T @ W @ y)\n",
    "\n",
    "    # Prediction at x0\n",
    "    x_vec = np.array([1, x])\n",
    "    y_pred.append(x_vec @ theta)\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label=\"Noisy Data\", alpha=0.6)\n",
    "plt.plot(X_test, y_pred, color='red', label=f\"LWR (tau={tau})\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Locally Weighted Regression with Gaussian Kernel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504cae49",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Reference Notebook\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Manual implementation of algorithms (not just sklearn)\n",
    "- Proper mathematical notation and variable naming\n",
    "- Visualization of results (parity plots, decision boundaries, etc.)\n",
    "- Evaluation metrics computed manually and with sklearn\n",
    "- Hyperparameter tuning with GridSearchCV\n",
    "- Cross-validation workflows\n",
    "- Code complexity level appropriate for intermediate ML course"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
