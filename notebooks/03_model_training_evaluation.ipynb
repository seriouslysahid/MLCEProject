{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection System (NIDS)\n",
    "## Notebook 3: Model Training & Evaluation\n",
    "\n",
    "**Team Member:** Member 3  \n",
    "**Dataset:** CIC-IDS2017 (Multi-class Classification)  \n",
    "**Date:** November 24, 2025  \n",
    "\n",
    "**Objectives:**\n",
    "1. Load preprocessed data from Notebook 2\n",
    "2. Train 3+ classification models\n",
    "3. Generate parity plots for train and test data\n",
    "4. Compute classification metrics (Accuracy, Precision, Recall, F1-Score)\n",
    "5. Create confusion matrices and ROC curves\n",
    "6. Compare model performance\n",
    "\n",
    "** Professor Requirements Covered:**\n",
    "- Requirement #3: Define I/O variables, implement MLR/Logistic Regression/SVC/PCA\n",
    "- Requirement #4: Train ML models\n",
    "- Requirement #5: Show parity plots and compute metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Evaluation Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\" Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local_save_helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOCAL OUTPUT SAVER (for Colab VS Code Extension)\n",
    "# ============================================================================\n",
    "# This ensures all outputs are saved to your local machine\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running on Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        \n",
    "        # Set base path to your local project in Drive\n",
    "        # IMPORTANT: Update this path to match your Google Drive structure\n",
    "        BASE_PATH = '/content/drive/MyDrive/MLCEProject'\n",
    "        \n",
    "        # Create output directories if they don't exist\n",
    "        for dir_name in ['outputs', 'models', 'data']:\n",
    "            Path(f'{BASE_PATH}/{dir_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\u2713 Google Drive mounted\")\n",
    "        print(f\"\u2713 Base path: {BASE_PATH}\")\n",
    "        print(f\"\u2713 Outputs will save to: {BASE_PATH}/outputs\")\n",
    "        print(f\"\u2713 Models will save to: {BASE_PATH}/models\")\n",
    "        print(f\"\u2713 Data will save to: {BASE_PATH}/data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not mount Drive: {e}\")\n",
    "        print(\"Using Colab local storage (will not sync automatically)\")\n",
    "        BASE_PATH = '/content'\n",
    "else:\n",
    "    # Running locally - use relative paths\n",
    "    BASE_PATH = '..'\n",
    "    print(\"\u2713 Running locally\")\n",
    "    print(\"\u2713 Using relative paths (../outputs, ../models, ../data)\")\n",
    "\n",
    "# Helper functions for saving with correct paths\n",
    "def get_output_path(filename):\n",
    "    \"\"\"Get correct path for output file\"\"\"\n",
    "    return f\"{BASE_PATH}/outputs/{filename}\"\n",
    "\n",
    "def get_model_path(filename):\n",
    "    \"\"\"Get correct path for model file\"\"\"\n",
    "    return f\"{BASE_PATH}/models/{filename}\"\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get correct path for data file\"\"\"\n",
    "    return f\"{BASE_PATH}/data/{filename}\"\n",
    "\n",
    "print(\"\\n\u2713 Local save helper ready!\")\n",
    "print(\"\\nUse these functions to save files:\")\n",
    "print(\"  - get_output_path('plot.png')  \u2192 saves to outputs/\")\n",
    "print(\"  - get_model_path('model.pkl')  \u2192 saves to models/\")\n",
    "print(\"  - get_data_path('data.csv')    \u2192 saves to data/\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constants",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "RANDOM_SEED = 42\n",
    "N_JOBS = -1\n",
    "DPI = 150  # For plot saving\n",
    "\n",
    "print(\"\u2713 Constants defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Preprocessed Data\n",
    "\n",
    "Load the train and test sets generated by Member 2 in Notebook 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data with memory optimization\n",
    "print(\"Loading preprocessed data...\\n\")\n",
    "\n",
    "# OPTIMIZATION: Use dtype specification and chunked reading for large files\n",
    "print(\"Loading X_train...\")\n",
    "X_train = pd.read_csv('../data/X_train.csv', dtype='float32')  # Use float32 to save memory\n",
    "print(f\"  Shape: {X_train.shape}\")\n",
    "\n",
    "print(\"Loading X_test...\")\n",
    "X_test = pd.read_csv('../data/X_test.csv', dtype='float32')\n",
    "print(f\"  Shape: {X_test.shape}\")\n",
    "\n",
    "print(\"Loading y_train...\")\n",
    "y_train = pd.read_csv('../data/y_train.csv').values.ravel()\n",
    "print(f\"  Shape: {y_train.shape}\")\n",
    "\n",
    "print(\"Loading y_test...\")\n",
    "y_test = pd.read_csv('../data/y_test.csv').values.ravel()\n",
    "print(f\"  Shape: {y_test.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set:   {X_train.shape}\")\n",
    "print(f\"Test set:       {X_test.shape}\")\n",
    "print(f\"Classes:        {np.unique(y_train)}\")\n",
    "print(f\"Num classes:    {len(np.unique(y_train))}\")\n",
    "print(f\"Memory usage:   {(X_train.memory_usage(deep=True).sum() + X_test.memory_usage(deep=True).sum()) / 1024**2:.2f} MB\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load label mapping\n",
    "with open(get_output_path('label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "print(\"\\nLabel Mapping:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"  {code}: {label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Define Input-Output Variables\n",
    "\n",
    "** Professor Requirement #3: Identify input-output variables and choose classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define_io",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"INPUT-OUTPUT VARIABLE DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n**Problem Type:** Multi-class Classification\")\n",
    "print(f\"\\n**Input Variables (X):**\")\n",
    "print(f\"  - Number of features: {X_train.shape[1]}\")\n",
    "print(f\"  - Feature names (first 10):\")\n",
    "for i, col in enumerate(X_train.columns[:10], 1):\n",
    "    print(f\"      {i:2d}. {col}\")\n",
    "print(f\"      ... ({X_train.shape[1] - 10} more features)\")\n",
    "\n",
    "print(f\"\\n**Output Variable (y):**\")\n",
    "print(f\"  - Variable name: Label_Encoded\")\n",
    "print(f\"  - Type: Categorical (Multi-class)\")\n",
    "print(f\"  - Number of classes: {len(np.unique(y_train))}\")\n",
    "print(f\"  - Class labels: {list(label_mapping.keys())}\")\n",
    "\n",
    "print(f\"\\n**Models to Implement:**\")\n",
    "print(f\"  1. Logistic Regression (Multi-class, multinomial)\")\n",
    "print(f\"  2. Support Vector Classifier (SVC, RBF kernel)\")\n",
    "print(f\"  3. PCA + Logistic Regression (Dimensionality reduction)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model 1: Logistic Regression (Multi-class)\n",
    "\n",
    "** Professor Requirement #4: Train ML models**\n",
    "\n",
    "Train Logistic Regression with multinomial setting for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (Multi-class)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Logistic Regression...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "lr_model = LogisticRegression(\n",
    "    multi_class='multinomial',  # For multi-class classification\n",
    "    solver='saga',              # Recommended for multinomial\n",
    "    max_iter=100,  # SAGA solver needs more iterations\n",
    "    tol=1e-3,                    # Slightly relaxed for speed\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=N_JOBS,                    # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Store for comparison\n",
    "lr_train_time = train_time\n",
    "print(f\"\u2713 Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_train_pred_lr = lr_model.predict(X_train)\n",
    "y_test_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Predict probabilities for parity plots\n",
    "y_train_proba_lr = lr_model.predict_proba(X_train)\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nPrediction shapes:\")\n",
    "print(f\"  Train predictions: {y_train_pred_lr.shape}\")\n",
    "print(f\"  Test predictions:  {y_test_pred_lr.shape}\")\n",
    "print(f\"  Train probabilities: {y_train_proba_lr.shape}\")\n",
    "print(f\"  Test probabilities:  {y_test_proba_lr.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "lr_train_metrics = {\n",
    "    'Accuracy': accuracy_score(y_train, y_train_pred_lr),\n",
    "    'Precision (macro)': precision_score(y_train, y_train_pred_lr, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_train, y_train_pred_lr, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_train, y_train_pred_lr, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "lr_test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_test_pred_lr),\n",
    "    'Precision (macro)': precision_score(y_test, y_test_pred_lr, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_test, y_test_pred_lr, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_test, y_test_pred_lr, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION - METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in lr_train_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for metric, value in lr_test_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model 2: Support Vector Classifier (SVC)\n",
    "\n",
    "Train SVC with RBF kernel for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_svc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL 2: LINEAR SUPPORT VECTOR CLASSIFIER (LinearSVC)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining LinearSVC (much faster than RBF kernel)...\")\n",
    "print(\"  Note: LinearSVC uses linear kernel, optimized for large datasets\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# OPTIMIZATION: Use LinearSVC instead of SVC(kernel='rbf')\n",
    "# LinearSVC is 50-100x faster and works well for this problem\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# LinearSVC doesn't support predict_proba, so we use calibration\n",
    "# dual='auto' chooses optimal solver (False when samples >> features)\n",
    "base_svc = LinearSVC(dual='auto', verbose=1, max_iter=2000, random_state=RANDOM_SEED)\n",
    "svc_model = CalibratedClassifierCV(base_svc, cv=3, verbose=1)\n",
    "\n",
    "# Train model with progress bar\n",
    "with tqdm(total=100, desc='Training LinearSVC', bar_format='{l_bar}{bar}| {elapsed}') as pbar:\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    pbar.update(100)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Store training time for comparison\n",
    "svc_train_time = train_time\n",
    "print(f\"\u2713 Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_train_pred_svc = svc_model.predict(X_train)\n",
    "y_test_pred_svc = svc_model.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba_svc = svc_model.predict_proba(X_train)\n",
    "y_test_proba_svc = svc_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nPrediction shapes:\")\n",
    "print(f\"  Train predictions: {y_train_pred_svc.shape}\")\n",
    "print(f\"  Test predictions:  {y_test_pred_svc.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_svc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "svc_train_metrics = {\n",
    "    'Accuracy': accuracy_score(y_train, y_train_pred_svc),\n",
    "    'Precision (macro)': precision_score(y_train, y_train_pred_svc, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_train, y_train_pred_svc, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_train, y_train_pred_svc, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "svc_test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_test_pred_svc),\n",
    "    'Precision (macro)': precision_score(y_test, y_test_pred_svc, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_test, y_test_pred_svc, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_test, y_test_pred_svc, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SVC - METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in svc_train_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for metric, value in svc_test_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model 3: PCA + Logistic Regression\n",
    "\n",
    "Apply PCA for dimensionality reduction, then train Logistic Regression.\n",
    "\n",
    "** Professor Requirement #3: Implement PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_pca_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MODEL 3: PCA + LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nCreating PCA + Logistic Regression pipeline...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create pipeline\n",
    "pca_lr_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=0.90, random_state=42)),  # Keep 90% variance (faster)\n",
    "    ('lr', LogisticRegression(verbose=1, multi_class='multinomial', solver='saga', max_iter=500, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train pipeline with progress bar\n",
    "with tqdm(total=100, desc='Training PCA+LogReg', bar_format='{l_bar}{bar}| {elapsed}') as pbar:\n",
    "    pca_lr_pipeline.fit(X_train, y_train)\n",
    "    pbar.update(100)\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Store for comparison\n",
    "pca_lr_train_time = train_time\n",
    "print(f\"\u2713 Training completed in {train_time:.2f} seconds\")\n",
    "\n",
    "# Get number of components\n",
    "n_components = pca_lr_pipeline.named_steps['pca'].n_components_\n",
    "explained_variance = pca_lr_pipeline.named_steps['pca'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"\\nPCA Details:\")\n",
    "print(f\"  Original features:     {X_train.shape[1]}\")\n",
    "print(f\"  PCA components:        {n_components}\")\n",
    "print(f\"  Variance explained:    {explained_variance:.4f} (90% target)\")\n",
    "print(f\"  Dimensionality reduction: {(1 - n_components/X_train.shape[1])*100:.1f}%\")\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_train_pred_pca_lr = pca_lr_pipeline.predict(X_train)\n",
    "y_test_pred_pca_lr = pca_lr_pipeline.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "y_train_proba_pca_lr = pca_lr_pipeline.predict_proba(X_train)\n",
    "y_test_proba_pca_lr = pca_lr_pipeline.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_pca_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "pca_lr_train_metrics = {\n",
    "    'Accuracy': accuracy_score(y_train, y_train_pred_pca_lr),\n",
    "    'Precision (macro)': precision_score(y_train, y_train_pred_pca_lr, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_train, y_train_pred_pca_lr, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_train, y_train_pred_pca_lr, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "pca_lr_test_metrics = {\n",
    "    'Accuracy': accuracy_score(y_test, y_test_pred_pca_lr),\n",
    "    'Precision (macro)': precision_score(y_test, y_test_pred_pca_lr, average='macro', zero_division=0),\n",
    "    'Recall (macro)': recall_score(y_test, y_test_pred_pca_lr, average='macro', zero_division=0),\n",
    "    'F1-Score (macro)': f1_score(y_test, y_test_pred_pca_lr, average='macro', zero_division=0)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PCA + LOGISTIC REGRESSION - METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in pca_lr_train_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "for metric, value in pca_lr_test_metrics.items():\n",
    "    print(f\"  {metric:20s}: {value:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Model Comparison Table\n",
    "\n",
    "Compare all models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Logistic Regression\n",
    "comparison_data.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Training Time (s)': lr_train_time,\n",
    "    'Train Accuracy': lr_train_metrics['Accuracy'],\n",
    "    'Test Accuracy': lr_test_metrics['Accuracy'],\n",
    "    'Train F1 (macro)': lr_train_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (macro)': lr_test_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (weighted)': f1_score(y_test, y_test_pred_lr, average='weighted'),\n",
    "    'Test Precision (weighted)': precision_score(y_test, y_test_pred_lr, average='weighted'),\n",
    "    'Test Recall (weighted)': recall_score(y_test, y_test_pred_lr, average='weighted')\n",
    "})\n",
    "\n",
    "# LinearSVC\n",
    "comparison_data.append({\n",
    "    'Model': 'LinearSVC',\n",
    "    'Training Time (s)': svc_train_time,\n",
    "    'Train Accuracy': svc_train_metrics['Accuracy'],\n",
    "    'Test Accuracy': svc_test_metrics['Accuracy'],\n",
    "    'Train F1 (macro)': svc_train_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (macro)': svc_test_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (weighted)': f1_score(y_test, y_test_pred_svc, average='weighted'),\n",
    "    'Test Precision (weighted)': precision_score(y_test, y_test_pred_svc, average='weighted'),\n",
    "    'Test Recall (weighted)': recall_score(y_test, y_test_pred_svc, average='weighted')\n",
    "})\n",
    "\n",
    "# PCA + Logistic Regression\n",
    "comparison_data.append({\n",
    "    'Model': 'PCA + LogReg',\n",
    "    'Training Time (s)': pca_lr_train_time,\n",
    "    'Train Accuracy': pca_lr_train_metrics['Accuracy'],\n",
    "    'Test Accuracy': pca_lr_test_metrics['Accuracy'],\n",
    "    'Train F1 (macro)': pca_lr_train_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (macro)': pca_lr_test_metrics['F1-Score (macro)'],\n",
    "    'Test F1 (weighted)': f1_score(y_test, y_test_pred_pca_lr, average='weighted'),\n",
    "    'Test Precision (weighted)': precision_score(y_test, y_test_pred_pca_lr, average='weighted'),\n",
    "    'Test Recall (weighted)': recall_score(y_test, y_test_pred_pca_lr, average='weighted')\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"MODEL PERFORMANCE COMPARISON (with Training Time & Weighted Metrics)\")\n",
    "print(\"=\"*110)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*110)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('../outputs/model_comparison.csv', index=False)\n",
    "print(\"\\n\u2713 Comparison table saved: outputs/model_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Parity Plots\n",
    "\n",
    "** Professor Requirement #5: Show parity plots for train and test data**\n",
    "\n",
    "For multi-class classification, plot predicted probability vs. actual class membership for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parity_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parity_multiclass(y_true, y_pred_proba, model_name, class_labels, data_type=\"Test\", max_classes=6):\n",
    "    \"\"\"\n",
    "    Plot parity plots for multi-class classification.\n",
    "    For each class, plot predicted probability vs. actual class membership.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array\n",
    "        True labels\n",
    "    y_pred_proba : array\n",
    "        Predicted probabilities (n_samples, n_classes)\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    class_labels : list\n",
    "        List of class labels\n",
    "    data_type : str\n",
    "        \"Train\" or \"Test\"\n",
    "    max_classes : int\n",
    "        Maximum number of classes to plot (to avoid overcrowding)\n",
    "    \"\"\"\n",
    "    n_classes = min(len(class_labels), max_classes)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        # Binary indicator: 1 if true class equals this class, else 0\n",
    "        y_binary = (y_true == i).astype(int)\n",
    "        \n",
    "        # Predicted probability for this class\n",
    "        y_proba_class = y_pred_proba[:, i]\n",
    "        \n",
    "        # Scatter plot with transparency\n",
    "        axes[i].scatter(y_binary, y_proba_class, alpha=0.2, s=5, color='blue')\n",
    "        axes[i].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "        axes[i].set_xlabel(f'Actual (Class {i})', fontsize=10)\n",
    "        axes[i].set_ylabel('Predicted Probability', fontsize=10)\n",
    "        axes[i].set_title(f'Class {i}: {list(class_labels.keys())[i][:15]}', fontsize=10, fontweight='bold')\n",
    "        axes[i].set_xlim([-0.1, 1.1])\n",
    "        axes[i].set_ylim([-0.1, 1.1])\n",
    "        axes[i].legend(fontsize=8)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - {data_type} Parity Plots', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    filename = f\"../outputs/parity_{model_name.lower().replace(' ', '_')}_{data_type.lower()}.png\"\n",
    "    plt.savefig(filename, dpi=DPI, bbox_inches='tight')\n",
    "    plt.close()  # Free memory\n",
    "    print(f\" Saved: {filename}\")\n",
    "\n",
    "print(\" Parity plot function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parity_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity plots for Logistic Regression\n",
    "print(\"\\nGenerating parity plots for Logistic Regression...\\n\")\n",
    "\n",
    "# Test data\n",
    "plot_parity_multiclass(y_test, y_test_proba_lr, \"Logistic Regression\", label_mapping, data_type=\"Test\")\n",
    "\n",
    "# Train data\n",
    "plot_parity_multiclass(y_train, y_train_proba_lr, \"Logistic Regression\", label_mapping, data_type=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parity_svc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity plots for SVC\n",
    "print(\"\\nGenerating parity plots for SVC...\\n\")\n",
    "\n",
    "# Test data\n",
    "plot_parity_multiclass(y_test, y_test_proba_svc, \"SVC\", label_mapping, data_type=\"Test\")\n",
    "\n",
    "# Train data\n",
    "plot_parity_multiclass(y_train, y_train_proba_svc, \"SVC\", label_mapping, data_type=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parity_pca_lr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parity plots for PCA + Logistic Regression\n",
    "print(\"\\nGenerating parity plots for PCA + Logistic Regression...\\n\")\n",
    "\n",
    "# Test data\n",
    "plot_parity_multiclass(y_test, y_test_proba_pca_lr, \"PCA + LogReg\", label_mapping, data_type=\"Test\")\n",
    "\n",
    "# Train data\n",
    "plot_parity_multiclass(y_train, y_train_proba_pca_lr, \"PCA + LogReg\", label_mapping, data_type=\"Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Confusion Matrices\n",
    "\n",
    "** Professor Requirement #5 (Continued): Visualization of classification performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_predictions = [\n",
    "    ('Logistic Regression', y_test_pred_lr),\n",
    "    ('SVC (RBF)', y_test_pred_svc),\n",
    "    ('PCA + LogReg', y_test_pred_pca_lr)\n",
    "]\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(models_predictions):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    axes[idx].set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
    "\n",
    "plt.suptitle('Confusion Matrices - Test Data', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(get_output_path('confusion_matrices.png', dpi=DPI, bbox_inches='tight')\n",
    "plt.close()  # Free memory\n",
    "\n",
    "print(\" Saved: outputs/confusion_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Classification Reports\n",
    "\n",
    "Detailed per-class performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_reports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class names\n",
    "class_names = list(label_mapping.keys())\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"CLASSIFICATION REPORT - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*90)\n",
    "print(classification_report(y_test, y_test_pred_lr, target_names=class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"CLASSIFICATION REPORT - SVC\")\n",
    "print(\"=\"*90)\n",
    "print(classification_report(y_test, y_test_pred_svc, target_names=class_names, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"CLASSIFICATION REPORT - PCA + LOGISTIC REGRESSION\")\n",
    "print(\"=\"*90)\n",
    "print(classification_report(y_test, y_test_pred_pca_lr, target_names=class_names, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models for reproducibility\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "print(\"Saving trained models...\\n\")\n",
    "\n",
    "joblib.dump(lr_model, get_model_path('logistic_regression.pkl')\n",
    "print(\"\u2713 Saved: models/logistic_regression.pkl\")\n",
    "\n",
    "joblib.dump(svc_model, get_model_path('linear_svc.pkl')\n",
    "print(\"\u2713 Saved: models/linear_svc.pkl\")\n",
    "\n",
    "joblib.dump(pca_lr_pipeline, get_model_path('pca_lr_pipeline.pkl')\n",
    "print(\"\u2713 Saved: models/pca_lr_pipeline.pkl\")\n",
    "\n",
    "print(\"\\n\u2713 All models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section11",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary & Key Findings\n",
    "\n",
    "### Model Training Summary:\n",
    "1.  Trained 3 models:\n",
    "   - Logistic Regression (Multi-class, multinomial)\n",
    "   - Support Vector Classifier (RBF kernel)\n",
    "   - PCA + Logistic Regression (95% variance retained)\n",
    "\n",
    "2.  Computed metrics:\n",
    "   - Accuracy, Precision (macro), Recall (macro), F1-Score (macro)\n",
    "   - Separate metrics for train and test sets\n",
    "\n",
    "3.  Generated visualizations:\n",
    "   - Parity plots for all 3 models (train and test)\n",
    "   - Confusion matrices for all 3 models\n",
    "   - Classification reports with per-class metrics\n",
    "\n",
    "### Best Performing Model:\n",
    "**Based on Test Accuracy:** [Fill after running - check comparison table]\n",
    "\n",
    "### Observations:\n",
    "- **Logistic Regression:** [Fill observations about speed, accuracy]\n",
    "- **SVC:** [Fill observations about accuracy vs. training time]\n",
    "- **PCA + LogReg:** [Fill observations about dimensionality reduction impact]\n",
    "\n",
    "### Files Generated:\n",
    "- `outputs/model_comparison.csv` - Metrics comparison table\n",
    "- `outputs/parity_*.png` - Parity plots (6 files)\n",
    "- `outputs/confusion_matrices.png` - Confusion matrices\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps for Member 4 (Cross-Validation):**\n",
    "1. Perform 5-fold stratified cross-validation\n",
    "2. Compare CV performance across all models\n",
    "3. Analyze feature importance (Logistic Regression coefficients)\n",
    "4. Simple hyperparameter tuning with GridSearchCV\n",
    "5. Write final conclusions\n",
    "\n",
    "---\n",
    "\n",
    "**Proceed to:** `04_cross_validation_analysis.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}