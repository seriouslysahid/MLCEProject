{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2f8359",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e80c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score, cross_validate, StratifiedKFold, learning_curve\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load preprocessed data\n",
    "# X_train = pd.read_csv('../data/X_train.csv')\n",
    "# X_test = pd.read_csv('../data/X_test.csv')\n",
    "# y_train = pd.read_csv('../data/y_train.csv').values.ravel()\n",
    "# y_test = pd.read_csv('../data/y_test.csv').values.ravel()\n",
    "\n",
    "# print(f\"Train set: {X_train.shape}\")\n",
    "# print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86394065",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 10-Fold Stratified Cross-Validation\n",
    "\n",
    "Evaluate model performance using k-fold cross-validation for robust estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02471d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define models\n",
    "# models = {\n",
    "#     'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "#     'SVC (RBF)': SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42),\n",
    "#     'ANN (MLP)': MLPClassifier(\n",
    "#         hidden_layer_sizes=(128, 64, 32),\n",
    "#         activation='relu',\n",
    "#         solver='adam',\n",
    "#         max_iter=200,\n",
    "#         random_state=42,\n",
    "#         early_stopping=True\n",
    "#     ),\n",
    "#     'PCA + SVC': Pipeline([\n",
    "#         ('pca', PCA(n_components=0.95, random_state=42)),\n",
    "#         ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "#     ])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d9d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define scoring metrics\n",
    "# scoring = {\n",
    "#     'accuracy': 'accuracy',\n",
    "#     'precision': make_scorer(precision_score),\n",
    "#     'recall': make_scorer(recall_score),\n",
    "#     'f1': make_scorer(f1_score),\n",
    "#     'roc_auc': 'roc_auc'\n",
    "# }\n",
    "\n",
    "# # Define cross-validation strategy\n",
    "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea6fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform cross-validation for all models\n",
    "# cv_results = {}\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\nPerforming 10-fold CV for {model_name}...\")\n",
    "#     start_time = time()\n",
    "#     \n",
    "#     # Perform cross-validation\n",
    "#     scores = cross_validate(\n",
    "#         model, X_train, y_train,\n",
    "#         cv=cv,\n",
    "#         scoring=scoring,\n",
    "#         return_train_score=True,\n",
    "#         n_jobs=-1  # Use all available cores\n",
    "#     )\n",
    "#     \n",
    "#     elapsed_time = time() - start_time\n",
    "#     \n",
    "#     # Store results\n",
    "#     cv_results[model_name] = {\n",
    "#         'accuracy_mean': scores['test_accuracy'].mean(),\n",
    "#         'accuracy_std': scores['test_accuracy'].std(),\n",
    "#         'precision_mean': scores['test_precision'].mean(),\n",
    "#         'precision_std': scores['test_precision'].std(),\n",
    "#         'recall_mean': scores['test_recall'].mean(),\n",
    "#         'recall_std': scores['test_recall'].std(),\n",
    "#         'f1_mean': scores['test_f1'].mean(),\n",
    "#         'f1_std': scores['test_f1'].std(),\n",
    "#         'roc_auc_mean': scores['test_roc_auc'].mean(),\n",
    "#         'roc_auc_std': scores['test_roc_auc'].std(),\n",
    "#         'cv_time': elapsed_time\n",
    "#     }\n",
    "#     \n",
    "#     print(f\"‚úÖ Completed in {elapsed_time:.2f} seconds\")\n",
    "#     print(f\"   Accuracy: {cv_results[model_name]['accuracy_mean']:.4f} (+/- {cv_results[model_name]['accuracy_std']:.4f})\")\n",
    "#     print(f\"   F1-Score: {cv_results[model_name]['f1_mean']:.4f} (+/- {cv_results[model_name]['f1_std']:.4f})\")\n",
    "#     print(f\"   ROC-AUC:  {cv_results[model_name]['roc_auc_mean']:.4f} (+/- {cv_results[model_name]['roc_auc_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1040b7",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cross-Validation Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72585ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create summary DataFrame\n",
    "# cv_summary = pd.DataFrame(cv_results).T\n",
    "# cv_summary = cv_summary.round(4)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*100)\n",
    "# print(\"10-FOLD CROSS-VALIDATION RESULTS\")\n",
    "# print(\"=\"*100)\n",
    "# print(cv_summary)\n",
    "# print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize cross-validation results with error bars\n",
    "# metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "# model_names = list(cv_results.keys())\n",
    "\n",
    "# fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "# axes = axes.ravel()\n",
    "\n",
    "# for idx, metric in enumerate(metrics):\n",
    "#     means = [cv_results[model][f'{metric}_mean'] for model in model_names]\n",
    "#     stds = [cv_results[model][f'{metric}_std'] for model in model_names]\n",
    "#     \n",
    "#     axes[idx].bar(range(len(model_names)), means, yerr=stds, \n",
    "#                   capsize=5, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'], alpha=0.8)\n",
    "#     axes[idx].set_xticks(range(len(model_names)))\n",
    "#     axes[idx].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "#     axes[idx].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "#     axes[idx].set_title(f'{metric.upper().replace(\"_\", \"-\")}', fontsize=13, fontweight='bold')\n",
    "#     axes[idx].set_ylim([0.5, 1.0])\n",
    "#     axes[idx].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# # Training time comparison\n",
    "# times = [cv_results[model]['cv_time'] for model in model_names]\n",
    "# axes[5].bar(range(len(model_names)), times, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'], alpha=0.8)\n",
    "# axes[5].set_xticks(range(len(model_names)))\n",
    "# axes[5].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "# axes[5].set_ylabel('Time (seconds)', fontsize=11, fontweight='bold')\n",
    "# axes[5].set_title('Cross-Validation Time', fontsize=13, fontweight='bold')\n",
    "# axes[5].grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../outputs/cv_results_comparison.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907c9ca",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Learning Curves Analysis\n",
    "\n",
    "Diagnose bias-variance tradeoff by plotting learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Function to plot learning curve\n",
    "# def plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10)):\n",
    "#     \"\"\"\n",
    "#     Plot learning curve to diagnose bias-variance tradeoff.\n",
    "#     \"\"\"\n",
    "#     train_sizes, train_scores, test_scores = learning_curve(\n",
    "#         estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,\n",
    "#         scoring='accuracy', random_state=42\n",
    "#     )\n",
    "#     \n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "#     \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                      train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "#     plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                      test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "#     plt.plot(train_sizes, train_scores_mean, 'o-', color='r', linewidth=2, label='Training score')\n",
    "#     plt.plot(train_sizes, test_scores_mean, 'o-', color='g', linewidth=2, label='Cross-validation score')\n",
    "#     \n",
    "#     plt.xlabel('Training Set Size', fontsize=13, fontweight='bold')\n",
    "#     plt.ylabel('Accuracy Score', fontsize=13, fontweight='bold')\n",
    "#     plt.title(f'Learning Curve - {title}', fontsize=15, fontweight='bold')\n",
    "#     plt.legend(loc='best', fontsize=11)\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'../outputs/learning_curve_{title.replace(\" \", \"_\").lower()}.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de48d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate learning curves for all models\n",
    "# print(\"Generating learning curves...\\n\")\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"Plotting learning curve for {model_name}...\")\n",
    "#     plot_learning_curve(model, model_name, X_train, y_train, cv=5)\n",
    "#     print(\"‚úÖ Completed\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e40765",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Identify most important features for model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Feature importance from Logistic Regression coefficients\n",
    "# print(\"Analyzing feature importance from Logistic Regression...\")\n",
    "\n",
    "# # Train LR model\n",
    "# lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_model.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importances (absolute coefficients)\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': X_train.columns,\n",
    "#     'importance': np.abs(lr_model.coef_[0])\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# print(\"\\nTop 20 Most Important Features:\")\n",
    "# print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685935ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize top features\n",
    "# top_n = 20\n",
    "# top_features = feature_importance.head(top_n)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "# plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "# plt.xlabel('Absolute Coefficient Value', fontsize=13, fontweight='bold')\n",
    "# plt.ylabel('Features', fontsize=13, fontweight='bold')\n",
    "# plt.title(f'Top {top_n} Most Important Features (Logistic Regression)', fontsize=15, fontweight='bold')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.grid(True, axis='x', alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('../outputs/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba107675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optional - Permutation importance (more robust but slower)\n",
    "# from sklearn.inspection import permutation_importance\n",
    "\n",
    "# print(\"Computing permutation importance...\")\n",
    "# perm_importance = permutation_importance(\n",
    "#     lr_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "# )\n",
    "\n",
    "# perm_importance_df = pd.DataFrame({\n",
    "#     'feature': X_train.columns,\n",
    "#     'importance': perm_importance.importances_mean\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# print(\"\\nTop 20 Features by Permutation Importance:\")\n",
    "# print(perm_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80932150",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Final Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify best model based on CV results\n",
    "# print(\"\\n\" + \"=\"*100)\n",
    "# print(\"FINAL RECOMMENDATIONS\")\n",
    "# print(\"=\"*100)\n",
    "\n",
    "# # Best by different metrics\n",
    "# best_accuracy = max(cv_results.items(), key=lambda x: x[1]['accuracy_mean'])\n",
    "# best_f1 = max(cv_results.items(), key=lambda x: x[1]['f1_mean'])\n",
    "# best_roc_auc = max(cv_results.items(), key=lambda x: x[1]['roc_auc_mean'])\n",
    "# fastest = min(cv_results.items(), key=lambda x: x[1]['cv_time'])\n",
    "\n",
    "# print(f\"\\nüèÜ Best Model by Accuracy: {best_accuracy[0]} ({best_accuracy[1]['accuracy_mean']:.4f})\")\n",
    "# print(f\"üèÜ Best Model by F1-Score: {best_f1[0]} ({best_f1[1]['f1_mean']:.4f})\")\n",
    "# print(f\"üèÜ Best Model by ROC-AUC: {best_roc_auc[0]} ({best_roc_auc[1]['roc_auc_mean']:.4f})\")\n",
    "# print(f\"‚ö° Fastest Model: {fastest[0]} ({fastest[1]['cv_time']:.2f}s)\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f456e4a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Analysis Completed:\n",
    "1. ‚úÖ 10-fold stratified cross-validation\n",
    "2. ‚úÖ Cross-validation results comparison\n",
    "3. ‚úÖ Learning curves for bias-variance diagnosis\n",
    "4. ‚úÖ Feature importance analysis\n",
    "\n",
    "### Key Insights:\n",
    "- **Model Selection:** [Fill in best model and justification]\n",
    "- **Bias-Variance:** [Summarize learning curve findings]\n",
    "- **Feature Importance:** [List top 5 most important features]\n",
    "- **Computational Efficiency:** [Compare training times]\n",
    "\n",
    "### Production Recommendations:\n",
    "1. **Recommended Model:** [Best model based on business requirements]\n",
    "2. **Reasoning:** Balance between performance, speed, and interpretability\n",
    "3. **Next Steps:** Hyperparameter tuning, ensemble methods, deployment strategy\n",
    "\n",
    "---\n",
    "**Project Complete! üéâ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
