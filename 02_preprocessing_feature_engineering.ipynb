{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection System (NIDS)\n",
    "## Notebook 2: Preprocessing & Feature Engineering\n",
    "\n",
    "**Team Member:** Member 2  \n",
    "**Dataset:** CIC-IDS2017 (Multi-class Classification)  \n",
    "**Date:** November 24, 2025  \n",
    "\n",
    "**Objectives:**\n",
    "1. Load combined dataset from Notebook 1\n",
    "2. Handle missing values\n",
    "3. Handle outliers (based on Member 1's analysis)\n",
    "4. Feature engineering (create 2-3 new features)\n",
    "5. Encode categorical variables\n",
    "6. Feature scaling (StandardScaler)\n",
    "7. Train-test split (70-30 stratified)\n",
    "8. Save processed data for modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(\" Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local_save_helper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOCAL OUTPUT SAVER (for Colab VS Code Extension)\n",
    "# ============================================================================\n",
    "# This ensures all outputs are saved to your local machine\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running on Colab\n",
    "IN_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "        \n",
    "        # Set base path to your local project in Drive\n",
    "        # IMPORTANT: Update this path to match your Google Drive structure\n",
    "        BASE_PATH = '/content/drive/MyDrive/MLCEProject'\n",
    "        \n",
    "        # Create output directories if they don't exist\n",
    "        for dir_name in ['outputs', 'models', 'data']:\n",
    "            Path(f'{BASE_PATH}/{dir_name}').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"\u2713 Google Drive mounted\")\n",
    "        print(f\"\u2713 Base path: {BASE_PATH}\")\n",
    "        print(f\"\u2713 Outputs will save to: {BASE_PATH}/outputs\")\n",
    "        print(f\"\u2713 Models will save to: {BASE_PATH}/models\")\n",
    "        print(f\"\u2713 Data will save to: {BASE_PATH}/data\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Could not mount Drive: {e}\")\n",
    "        print(\"Using Colab local storage (will not sync automatically)\")\n",
    "        BASE_PATH = '/content'\n",
    "else:\n",
    "    # Running locally - use relative paths\n",
    "    BASE_PATH = '..'\n",
    "    print(\"\u2713 Running locally\")\n",
    "    print(\"\u2713 Using relative paths (../outputs, ../models, ../data)\")\n",
    "\n",
    "# Helper functions for saving with correct paths\n",
    "def get_output_path(filename):\n",
    "    \"\"\"Get correct path for output file\"\"\"\n",
    "    return f\"{BASE_PATH}/outputs/{filename}\"\n",
    "\n",
    "def get_model_path(filename):\n",
    "    \"\"\"Get correct path for model file\"\"\"\n",
    "    return f\"{BASE_PATH}/models/{filename}\"\n",
    "\n",
    "def get_data_path(filename):\n",
    "    \"\"\"Get correct path for data file\"\"\"\n",
    "    return f\"{BASE_PATH}/data/{filename}\"\n",
    "\n",
    "print(\"\\n\u2713 Local save helper ready!\")\n",
    "print(\"\\nUse these functions to save files:\")\n",
    "print(\"  - get_output_path('plot.png')  \u2192 saves to outputs/\")\n",
    "print(\"  - get_model_path('model.pkl')  \u2192 saves to models/\")\n",
    "print(\"  - get_data_path('data.csv')    \u2192 saves to data/\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Combined Dataset\n",
    "\n",
    "Load the combined CIC-IDS2017 dataset (same as Notebook 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files. Loading with PyArrow engine...\n",
      "Reading: Tuesday-WorkingHours.pcap_ISCX.csv...  Shape: (445909, 78) \u2713\n",
      "Reading: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv...  Shape: (288602, 78) \u2713\n",
      "Reading: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv...  Shape: (170366, 78) \u2713\n",
      "Reading: Monday-WorkingHours.pcap_ISCX.csv...  Shape: (529918, 78) \u2713\n",
      "Reading: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv...  Shape: (225745, 78) \u2713\n",
      "Reading: Friday-WorkingHours-Morning.pcap_ISCX.csv...  Shape: (191033, 78) \u2713\n",
      "Reading: Wednesday-workingHours.pcap_ISCX.csv...  Shape: (692703, 78) \u2713\n",
      "Reading: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv...  Shape: (286467, 78) \u2713\n",
      "\n",
      "Combining dataframes...\n",
      "Cleaning 'Label' column... \u2713\n",
      "\n",
      " Dataset loaded\n",
      "Shape: (2830743, 78)\n",
      "Memory: 807.94 MB\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Dataset Verification Function (from Notebook 1)\n",
    "def verify_dataset(path):\n",
    "    \"\"\"Verify dataset existence and validity\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return False, []\n",
    "        \n",
    "    csv_files = list(path.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        return False, []\n",
    "        \n",
    "    # Check total size (should be > 100MB for full dataset)\n",
    "    total_size = sum(f.stat().st_size for f in csv_files) / (1024**2)\n",
    "    if total_size < 10:\n",
    "        print(f\"Warning: Dataset seems too small ({total_size:.2f} MB)\")\n",
    "        \n",
    "    return True, csv_files\n",
    "\n",
    "# OPTIMIZATION: Function to reduce memory usage (Robust Version)\n",
    "def optimize_dtypes(df):\n",
    "    # Handle duplicate columns if any (keep first)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    \n",
    "    # Downcast floats to float32\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        try:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    # Downcast integers\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    for col in int_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        except Exception:\n",
    "            pass\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Load all CSV files and combine with OPTIMIZATIONS\n",
    "data_path = Path('../data/CICIDS2017/')\n",
    "\n",
    "# Verify dataset first\n",
    "valid, csv_files = verify_dataset(data_path)\n",
    "\n",
    "if not valid:\n",
    "    raise FileNotFoundError(f\"Dataset not found or invalid in {data_path}. Please run Notebook 1 first.\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files. Loading with PyArrow engine...\")\n",
    "\n",
    "dfs = []\n",
    "failed_files = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        print(f\"Reading: {file.name}...\", end=\" \")\n",
    "        # Try using pyarrow engine for faster reading\n",
    "        try:\n",
    "            df_temp = pd.read_csv(file, engine='pyarrow')\n",
    "        except:\n",
    "            df_temp = pd.read_csv(file) # Fallback\n",
    "        \n",
    "        # Optimize memory immediately\n",
    "        df_temp = optimize_dtypes(df_temp)\n",
    "        \n",
    "        dfs.append(df_temp)\n",
    "        print(f\" Shape: {df_temp.shape} \u2713\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" FAILED! Error: {e}\")\n",
    "        failed_files.append(file)\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No data was loaded successfully!\")\n",
    "\n",
    "print(\"\\nCombining dataframes...\")\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# OPTIMIZATION: Drop unnecessary columns early (saves memory)\n",
    "unnecessary_cols = ['Source IP', 'Destination IP', 'Timestamp']\n",
    "df = df.drop(columns=[c for c in unnecessary_cols if c in df.columns], errors='ignore')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# CLEANUP: Fix encoding issues in 'Label' column (same as Notebook 1)\n",
    "if 'Label' in df.columns:\n",
    "    print(\"Cleaning 'Label' column...\", end=\" \")\n",
    "    df['Label'] = df['Label'].str.replace('\\ufffd', '-', regex=False)\n",
    "    df['Label'] = df['Label'].str.strip()\n",
    "    print(\"\u2713\")\n",
    "\n",
    "print(f\"\\n Dataset loaded\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "identify_target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column: 'Label'\n",
      "Classes: ['BENIGN' 'FTP-Patator' 'SSH-Patator' 'Infiltration'\n",
      " 'Web Attack - Brute Force' 'Web Attack - XSS'\n",
      " 'Web Attack - Sql Injection' 'DDoS' 'Bot' 'DoS slowloris'\n",
      " 'DoS Slowhttptest' 'DoS Hulk' 'DoS GoldenEye' 'Heartbleed' 'PortScan']\n"
     ]
    }
   ],
   "source": [
    "# Identify target column\n",
    "target_col = ' Label' if ' Label' in df.columns else 'Label'\n",
    "print(f\"Target column: '{target_col}'\")\n",
    "print(f\"Classes: {df[target_col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Handle Missing Values\n",
    "\n",
    "Check for missing values and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "missing_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "              Missing Count  Percentage\n",
      "Flow Bytes/s           1358      0.0480\n"
     ]
    }
   ],
   "source": [
    "# Check missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "cols_with_missing = missing_df[missing_df['Missing Count'] > 0]\n",
    "\n",
    "print(\"Columns with missing values:\")\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(cols_with_missing)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "missing_handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputing 1 columns with median\n",
      "\n",
      "\u2713 Missing values handled\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "# Strategy: Drop columns with >50% missing, impute others with median\n",
    "\n",
    "if len(cols_with_missing) > 0:\n",
    "    # Drop columns with >50% missing\n",
    "    high_missing_cols = cols_with_missing[cols_with_missing['Percentage'] > 50].index.tolist()\n",
    "    if len(high_missing_cols) > 0:\n",
    "        print(f\"\\nDropping {len(high_missing_cols)} columns with >50% missing values\")\n",
    "        df = df.drop(columns=high_missing_cols)\n",
    "    \n",
    "    # Impute remaining missing values with median\n",
    "    remaining_missing = df.columns[df.isnull().any()].tolist()\n",
    "    if target_col in remaining_missing:\n",
    "        remaining_missing.remove(target_col)\n",
    "    \n",
    "    if len(remaining_missing) > 0:\n",
    "        print(f\"\\nImputing {len(remaining_missing)} columns with median\")\n",
    "        \n",
    "        # OPTIMIZATION: Pre-compute all medians at once (faster)\n",
    "        medians = df[remaining_missing].median()\n",
    "        for col in remaining_missing:\n",
    "            df[col] = df[col].fillna(medians[col])\n",
    "    \n",
    "    print(f\"\\n\u2713 Missing values handled\")\n",
    "    print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"\\n\u2713 No missing values to handle\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Handle Outliers\n",
    "\n",
    "Based on Member 1's outlier analysis, handle outliers using capping method (Winsorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outlier_handle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with >10% outliers: 49\n",
      "\n",
      "Capping outliers at 1st and 99th percentiles (VECTORIZED)...\n",
      "\n",
      "\u2713 Outliers capped for 49 features\n"
     ]
    }
   ],
   "source": [
    "# Load outlier summary from Notebook 1\n",
    "outlier_path = Path('../outputs/outlier_summary.csv')\n",
    "\n",
    "if outlier_path.exists():\n",
    "    outlier_summary = pd.read_csv(outlier_path)\n",
    "    \n",
    "    # Select features with >10% outliers for capping\n",
    "    features_to_cap = outlier_summary[outlier_summary['Percentage'] > 10]['Feature'].tolist()\n",
    "    \n",
    "    print(f\"Features with >10% outliers: {len(features_to_cap)}\")\n",
    "    print(f\"\\nCapping outliers at 1st and 99th percentiles (VECTORIZED)...\")\n",
    "    \n",
    "    # OPTIMIZATION: Vectorized capping (5-10x faster)\n",
    "    # Compute percentiles for all features at once\n",
    "    p01 = df[features_to_cap].quantile(0.01)\n",
    "    p99 = df[features_to_cap].quantile(0.99)\n",
    "    \n",
    "    # Clip all features at once\n",
    "    df[features_to_cap] = df[features_to_cap].clip(lower=p01, upper=p99, axis=1)\n",
    "    \n",
    "    print(f\"\\n\u2713 Outliers capped for {len(features_to_cap)} features\")\n",
    "else:\n",
    "    print(\"\\nWarning: outlier_summary.csv not found. Skipping outlier capping.\")\n",
    "    print(\"Please run Notebook 1 to generate outlier analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering\n",
    "\n",
    "Create 2-3 new features based on domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new features...\n",
      "\n",
      " Created: Packet_Rate = Total Fwd Packets / Flow Duration\n",
      " Created: Byte_Rate = Total Length of Fwd Packets / Flow Duration\n",
      " Created: Packet_Size_Ratio = Total Length of Fwd Packets / Total Length of Bwd Packets\n",
      "\n",
      "New feature count: 3\n",
      "Total features: 81\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: Create new features\n",
    "print(\"Creating new features...\\n\")\n",
    "\n",
    "# Feature 1: Packet Rate (packets per second)\n",
    "if 'Flow Duration' in df.columns and 'Total Fwd Packets' in df.columns:\n",
    "    df['Packet_Rate'] = df['Total Fwd Packets'] / (df['Flow Duration'] + 1)  # +1 to avoid division by zero\n",
    "    print(\" Created: Packet_Rate = Total Fwd Packets / Flow Duration\")\n",
    "\n",
    "# Feature 2: Byte Rate (bytes per second)\n",
    "if 'Flow Duration' in df.columns and 'Total Length of Fwd Packets' in df.columns:\n",
    "    df['Byte_Rate'] = df['Total Length of Fwd Packets'] / (df['Flow Duration'] + 1)\n",
    "    print(\" Created: Byte_Rate = Total Length of Fwd Packets / Flow Duration\")\n",
    "\n",
    "# Feature 3: Packet Size Ratio (forward/backward)\n",
    "if 'Total Length of Fwd Packets' in df.columns and 'Total Length of Bwd Packets' in df.columns:\n",
    "    df['Packet_Size_Ratio'] = df['Total Length of Fwd Packets'] / (df['Total Length of Bwd Packets'] + 1)\n",
    "    print(\" Created: Packet_Size_Ratio = Total Length of Fwd Packets / Total Length of Bwd Packets\")\n",
    "\n",
    "print(f\"\\nNew feature count: 3\")\n",
    "print(f\"Total features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Encode Categorical Variables\n",
    "\n",
    "** Professor Requirement #3 (Partial): Prepare I/O variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "encode_target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping:\n",
      "  0: BENIGN\n",
      "  1: Bot\n",
      "  2: DDoS\n",
      "  3: DoS GoldenEye\n",
      "  4: DoS Hulk\n",
      "  5: DoS Slowhttptest\n",
      "  6: DoS slowloris\n",
      "  7: FTP-Patator\n",
      "  8: Heartbleed\n",
      "  9: Infiltration\n",
      "  10: PortScan\n",
      "  11: SSH-Patator\n",
      "  12: Web Attack - Brute Force\n",
      "  13: Web Attack - Sql Injection\n",
      "  14: Web Attack - XSS\n",
      "\n",
      " Label mapping saved: outputs/label_mapping.json\n"
     ]
    }
   ],
   "source": [
    "# Encode target variable (attack labels)\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label_Encoded'] = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "# Save label mapping\n",
    "# Create mapping with standard Python types for JSON serialization\n",
    "label_mapping = {str(label): int(code) for label, code in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
    "print(\"Label Encoding Mapping:\")\n",
    "for label, code in label_mapping.items():\n",
    "    print(f\"  {code}: {label}\")\n",
    "\n",
    "# Save mapping to file\n",
    "import json\n",
    "with open(get_output_path('label_mapping.json', 'w') as f:\n",
    "    json.dump(label_mapping, f, indent=2)\n",
    "\n",
    "print(\"\\n Label mapping saved: outputs/label_mapping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "check_categorical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other categorical columns: []\n",
      "\n",
      " No additional categorical columns to encode\n"
     ]
    }
   ],
   "source": [
    "# Check for other categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target column from categorical list\n",
    "if target_col in categorical_cols:\n",
    "    categorical_cols.remove(target_col)\n",
    "\n",
    "print(f\"Other categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Handle if any exist (e.g., protocol types)\n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nEncoding categorical features...\")\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "        print(f\"   Encoded: {col}\")\n",
    "else:\n",
    "    print(\"\\n No additional categorical columns to encode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Separate Features and Target\n",
    "\n",
    "** Professor Requirement #3: Identify input-output variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "separate_xy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INPUT-OUTPUT VARIABLES DEFINITION\n",
      "======================================================================\n",
      "Input Variables (X): 80 features\n",
      "Output Variable (y): Label_Encoded (multi-class, 15 classes)\n",
      "Total Samples: 2,830,743\n",
      "======================================================================\n",
      "\n",
      "Feature names (first 10):\n",
      "   1. Destination Port\n",
      "   2. Flow Duration\n",
      "   3. Total Fwd Packets\n",
      "   4. Total Backward Packets\n",
      "   5. Total Length of Fwd Packets\n",
      "   6. Total Length of Bwd Packets\n",
      "   7. Fwd Packet Length Max\n",
      "   8. Fwd Packet Length Min\n",
      "   9. Fwd Packet Length Mean\n",
      "  10. Fwd Packet Length Std\n",
      "  ... (70 more features)\n",
      "Memory cleared: df deleted\n"
     ]
    }
   ],
   "source": [
    "# Define X (features) and y (target)\n",
    "# Exclude original label column and encoded label column\n",
    "exclude_cols = [target_col, 'Label_Encoded']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['Label_Encoded']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INPUT-OUTPUT VARIABLES DEFINITION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Input Variables (X): {X.shape[1]} features\")\n",
    "print(f\"Output Variable (y): {y.name} (multi-class, {y.nunique()} classes)\")\n",
    "print(f\"Total Samples: {len(X):,}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFeature names (first 10):\")\n",
    "for i, col in enumerate(X.columns[:10], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "print(f\"  ... ({X.shape[1] - 10} more features)\")\n",
    "\n",
    "# OPTIMIZATION: Clear memory\n",
    "import gc\n",
    "del df\n",
    "gc.collect()\n",
    "print(\"Memory cleared: df deleted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Train-Test Split\n",
    "\n",
    "** Professor Requirement #4: Split data into train and test**\n",
    "\n",
    "Perform 70-30 stratified split to maintain class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified train-test split (70-30)\n",
    "print(\"Performing train-test split (70-30 stratified)...\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Features:       {X_train.shape[1]}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nClass distribution verification:\")\n",
    "print(\"\\nOriginal:\")\n",
    "print(y.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTraining:\")\n",
    "print(y_train.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest:\")\n",
    "print(y_test.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\n Stratified split maintains class distribution\")\n",
    "\n",
    "# OPTIMIZATION: Clear memory\n",
    "del X, y\n",
    "gc.collect()\n",
    "print(\"Memory cleared: X and y deleted\")\n",
    "\n",
    "# Save feature names for later use\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"\\nSaved {len(feature_names)} feature names for CSV export\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Scaling\n",
    "\n",
    "Apply StandardScaler to normalize features (zero mean, unit variance).\n",
    "\n",
    "**Important:** Fit scaler on training data only, transform both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "scaling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling inf/NaN values before scaling...\n",
      "\u2713 Handled inf/NaN values\n",
      "\n",
      "Applying feature scaling (StandardScaler) with CHUNKING...\n",
      "\n",
      "Fitting scaler on 1,981,520 samples in chunks of 50,000...\n",
      "  Processed 50,000/1,981,520 samples...\n",
      "  Processed 550,000/1,981,520 samples...\n",
      "  Processed 1,050,000/1,981,520 samples...\n",
      "  Processed 1,550,000/1,981,520 samples...\n",
      "\u2713 Scaler fitted!\n",
      "\n",
      "Transforming training data...\n",
      "\u2713 Training data scaled\n",
      "\n",
      "Transforming test data...\n",
      "\u2713 Test data scaled\n",
      "\n",
      "\u2713 Feature scaling complete\n",
      "Scaled training shape: (1981520, 80)\n",
      "Scaled test shape: (849223, 80)\n"
     ]
    }
   ],
   "source": [
    "# Handle inf/NaN values BEFORE scaling (critical for preventing crashes)\n",
    "print(\"Handling inf/NaN values before scaling...\")\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill NaN with median (simple and effective)\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_train.median())  # Use training median for test\n",
    "print(f\"\u2713 Handled inf/NaN values\\n\")\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Applying feature scaling (StandardScaler) with CHUNKING...\\n\")\n",
    "\n",
    "# OPTIMIZATION: Use smaller chunks for safety\n",
    "chunk_size = 50000  # Reduced from 100000\n",
    "total_samples = X_train.shape[0]\n",
    "\n",
    "print(f\"Fitting scaler on {total_samples:,} samples in chunks of {chunk_size:,}...\")\n",
    "\n",
    "# Fit on training data in chunks\n",
    "for i in range(0, total_samples, chunk_size):\n",
    "    end = min(i + chunk_size, total_samples)\n",
    "    chunk = X_train.iloc[i:end]\n",
    "    scaler.partial_fit(chunk)\n",
    "    if (i // chunk_size) % 10 == 0:\n",
    "        print(f\"  Processed {end:,}/{total_samples:,} samples...\")\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\u2713 Scaler fitted!\\n\")\n",
    "\n",
    "# Transform training data in chunks\n",
    "print(\"Transforming training data...\")\n",
    "X_train_scaled_list = []\n",
    "for i in range(0, total_samples, chunk_size):\n",
    "    end = min(i + chunk_size, total_samples)\n",
    "    chunk = X_train.iloc[i:end]\n",
    "    X_train_scaled_list.append(scaler.transform(chunk))\n",
    "    del chunk\n",
    "    if (i // chunk_size) % 10 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "X_train_scaled = np.vstack(X_train_scaled_list)\n",
    "del X_train_scaled_list, X_train\n",
    "gc.collect()\n",
    "print(\"\u2713 Training data scaled\\n\")\n",
    "\n",
    "# Transform test data in chunks\n",
    "print(\"Transforming test data...\")\n",
    "total_test = X_test.shape[0]\n",
    "X_test_scaled_list = []\n",
    "for i in range(0, total_test, chunk_size):\n",
    "    end = min(i + chunk_size, total_test)\n",
    "    chunk = X_test.iloc[i:end]\n",
    "    X_test_scaled_list.append(scaler.transform(chunk))\n",
    "    del chunk\n",
    "    if (i // chunk_size) % 10 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "X_test_scaled = np.vstack(X_test_scaled_list)\n",
    "del X_test_scaled_list, X_test\n",
    "gc.collect()\n",
    "print(\"\u2713 Test data scaled\\n\")\n",
    "\n",
    "print(f\"\u2713 Feature scaling complete\")\n",
    "print(f\"Scaled training shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test shape: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Processed Data\n",
    "\n",
    "Save train and test sets for use in Notebook 3 (Model Training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files\n",
    "print(\"Saving processed data to CSV files...\\n\")\n",
    "\n",
    "# Convert numpy arrays to DataFrames first\n",
    "print(\"Converting to DataFrames...\")\n",
    "# Convert to DataFrames with original feature names\n",
    "X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "print(\"\u2713\\n\")\n",
    "\n",
    "# OPTIMIZATION: Use float32 and fixed precision to speed up writing\n",
    "print(\"Saving X_train.csv (this may take a few minutes)...\", end=\" \")\n",
    "X_train_df.astype('float32').to_csv(get_data_path('X_train.csv', index=False, float_format='%.4f')\n",
    "print(\"\u2713\")\n",
    "\n",
    "print(\"Saving X_test.csv...\", end=\" \")\n",
    "X_test_df.astype('float32').to_csv(get_data_path('X_test.csv', index=False, float_format='%.4f')\n",
    "print(\"\u2713\")\n",
    "\n",
    "print(\"Saving y_train.csv...\", end=\" \")\n",
    "y_train.to_csv(get_data_path('y_train.csv', index=False, header=['Label_Encoded'])\n",
    "print(\"\u2713\")\n",
    "\n",
    "print(\"Saving y_test.csv...\", end=\" \")\n",
    "y_test.to_csv(get_data_path('y_test.csv', index=False, header=['Label_Encoded'])\n",
    "print(\"\u2713\")\n",
    "\n",
    "print(\"\\n\u2713 All processed data saved successfully!\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - data/X_train.csv: {X_train_df.shape[0]:,} rows \u00d7 {X_train_df.shape[1]} columns\")\n",
    "print(f\"  - data/X_test.csv: {X_test_df.shape[0]:,} rows \u00d7 {X_test_df.shape[1]} columns\")\n",
    "print(f\"  - data/y_train.csv: {len(y_train):,} rows\")\n",
    "print(f\"  - data/y_test.csv: {len(y_test):,} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section11",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Preprocessing Summary\n",
    "\n",
    "### Actions Performed:\n",
    "1.  Loaded CIC-IDS2017 dataset ([fill number] records)\n",
    "2.  Handled missing values (dropped/imputed as needed)\n",
    "3.  Capped outliers at 1st and 99th percentiles ([fill number] features)\n",
    "4.  Created 3 new features (Packet_Rate, Byte_Rate, Packet_Size_Ratio)\n",
    "5.  Encoded target variable (multi-class: [fill number] classes)\n",
    "6.  Defined X ([fill number] features) and y (Label_Encoded)\n",
    "7.  Performed 70-30 stratified train-test split\n",
    "8.  Applied StandardScaler (fit on train, transform train+test)\n",
    "9.  Saved processed data to CSV files\n",
    "\n",
    "### Dataset Summary:\n",
    "- **Training samples:** [fill from output]\n",
    "- **Test samples:** [fill from output]\n",
    "- **Features:** [fill from output]\n",
    "- **Classes:** [fill from label_mapping]\n",
    "- **Class balance:** Maintained through stratified splitting\n",
    "\n",
    "### Files Generated:\n",
    "- `data/X_train.csv` - Scaled training features\n",
    "- `data/X_test.csv` - Scaled test features\n",
    "- `data/y_train.csv` - Training labels\n",
    "- `data/y_test.csv` - Test labels\n",
    "- `outputs/label_mapping.json` - Label encoding reference\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps for Member 3 (Modeling):**\n",
    "1. Load preprocessed data from CSV files\n",
    "2. Train 3+ models (Logistic Regression, SVC, PCA+LogReg)\n",
    "3. Generate parity plots for train and test\n",
    "4. Compute classification metrics\n",
    "5. Create confusion matrices and ROC curves\n",
    "\n",
    "---\n",
    "\n",
    "**Proceed to:** `03_model_training_evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}