{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e5e9c1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c48752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Utilities\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "# data_files = glob.glob('../data/UNSW-NB15_*.csv')\n",
    "# df = pd.concat([pd.read_csv(file) for file in sorted(data_files)], ignore_index=True)\n",
    "# print(f\"Loaded {len(df):,} records with {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a328be60",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74759d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check missing values again\n",
    "# missing_summary = df.isnull().sum()\n",
    "# print(\"Missing values per column:\")\n",
    "# print(missing_summary[missing_summary > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Strategy for handling missing values\n",
    "# Option 1: Drop columns with >50% missing\n",
    "# threshold = 0.5\n",
    "# cols_to_drop = missing_summary[missing_summary > len(df) * threshold].index\n",
    "# df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# Option 2: Impute numerical features with median\n",
    "# numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# df[numerical_cols] = imputer.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Option 3: Impute categorical features with mode\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "# for col in categorical_cols:\n",
    "#     df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "# print(\"✅ Missing values handled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b5261",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf76d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define outlier treatment strategy\n",
    "# Option 1: Capping (Winsorization) - clip to percentiles\n",
    "# def cap_outliers(data, column, lower_percentile=0.01, upper_percentile=0.99):\n",
    "#     lower_cap = data[column].quantile(lower_percentile)\n",
    "#     upper_cap = data[column].quantile(upper_percentile)\n",
    "#     data[column] = data[column].clip(lower=lower_cap, upper=upper_cap)\n",
    "#     return data\n",
    "\n",
    "# # Apply to selected numerical features\n",
    "# features_to_cap = ['dur', 'sbytes', 'dbytes', 'spkts', 'dpkts']  # example\n",
    "# for col in features_to_cap:\n",
    "#     if col in df.columns:\n",
    "#         df = cap_outliers(df, col)\n",
    "\n",
    "# Option 2: Log transformation for skewed features\n",
    "# skewed_features = ['sbytes', 'dbytes']  # example\n",
    "# for col in skewed_features:\n",
    "#     if col in df.columns:\n",
    "#         df[f'{col}_log'] = np.log1p(df[col])  # log1p handles zeros\n",
    "\n",
    "# print(\"✅ Outliers treated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae129919",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering\n",
    "\n",
    "Create new features that might improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd998c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Engineer new features\n",
    "\n",
    "# Example 1: Packet rate\n",
    "# if 'spkts' in df.columns and 'dur' in df.columns:\n",
    "#     df['spkts_rate'] = df['spkts'] / (df['dur'] + 1e-6)  # avoid division by zero\n",
    "#     df['dpkts_rate'] = df['dpkts'] / (df['dur'] + 1e-6)\n",
    "\n",
    "# Example 2: Byte ratios\n",
    "# if 'sbytes' in df.columns and 'dbytes' in df.columns:\n",
    "#     df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)\n",
    "\n",
    "# Example 3: Total traffic volume\n",
    "# df['total_bytes'] = df['sbytes'] + df['dbytes']\n",
    "# df['total_pkts'] = df['spkts'] + df['dpkts']\n",
    "\n",
    "# Example 4: Average packet size\n",
    "# df['avg_pkt_size'] = df['total_bytes'] / (df['total_pkts'] + 1)\n",
    "\n",
    "# Example 5: Time-based features\n",
    "# if 'stime' in df.columns:\n",
    "#     df['hour'] = pd.to_datetime(df['stime'], unit='s').dt.hour\n",
    "#     df['is_night'] = df['hour'].apply(lambda x: 1 if x >= 22 or x <= 6 else 0)\n",
    "\n",
    "# print(\"✅ Feature engineering completed\")\n",
    "# print(f\"New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92630ca",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Identify categorical columns\n",
    "# categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "# # Exclude target-related columns if present\n",
    "# categorical_cols = [col for col in categorical_cols if col not in ['attack_cat']]\n",
    "# print(f\"Categorical columns to encode: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb946d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode categorical variables\n",
    "\n",
    "# Option 1: Label Encoding for ordinal or low-cardinality features\n",
    "# label_encoders = {}\n",
    "# for col in categorical_cols:\n",
    "#     le = LabelEncoder()\n",
    "#     df[col] = le.fit_transform(df[col].astype(str))\n",
    "#     label_encoders[col] = le\n",
    "\n",
    "# Option 2: One-Hot Encoding for nominal features\n",
    "# df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# print(\"✅ Categorical variables encoded\")\n",
    "# print(f\"Final shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ed6cc",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Feature Scaling\n",
    "\n",
    "Normalize features for algorithms sensitive to scale (SVC, ANN, Logistic Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc41f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Separate features and target\n",
    "# X = df.drop(columns=['label', 'attack_cat'], errors='ignore')\n",
    "# y = df['label']\n",
    "\n",
    "# # Remove id column if exists\n",
    "# if 'id' in X.columns:\n",
    "#     X = X.drop(columns=['id'])\n",
    "\n",
    "# print(f\"Feature matrix shape: {X.shape}\")\n",
    "# print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Note - Scaling will be done AFTER train-test split to prevent data leakage\n",
    "# We'll scale in the next notebook during model training\n",
    "# For now, just verify data is ready\n",
    "\n",
    "# print(\"Feature names:\")\n",
    "# print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c010b1d",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Train-Test Split (70-30 Stratified)\n",
    "\n",
    "Split data while maintaining class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Stratified train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, \n",
    "#     test_size=0.30, \n",
    "#     random_state=42, \n",
    "#     stratify=y\n",
    "# )\n",
    "\n",
    "# print(\"Train set size:\", X_train.shape)\n",
    "# print(\"Test set size:\", X_test.shape)\n",
    "# print(\"\\nTrain set class distribution:\")\n",
    "# print(y_train.value_counts(normalize=True))\n",
    "# print(\"\\nTest set class distribution:\")\n",
    "# print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2695315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply scaling to train and test sets separately\n",
    "# scaler = StandardScaler()\n",
    "# # OR use RobustScaler if data has outliers:\n",
    "# # scaler = RobustScaler()\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)  # Use fitted scaler, don't fit again!\n",
    "\n",
    "# # Convert back to DataFrame for easier handling\n",
    "# X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "# X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# print(\"✅ Feature scaling completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ae6ef",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d02163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save processed datasets for modeling\n",
    "# X_train_scaled.to_csv('../data/X_train.csv', index=False)\n",
    "# X_test_scaled.to_csv('../data/X_test.csv', index=False)\n",
    "# y_train.to_csv('../data/y_train.csv', index=False)\n",
    "# y_test.to_csv('../data/y_test.csv', index=False)\n",
    "\n",
    "# # Save scaler for future use\n",
    "# joblib.dump(scaler, '../models/scaler.pkl')\n",
    "\n",
    "# print(\"✅ Processed data saved to /data directory\")\n",
    "# print(\"✅ Scaler saved to /models directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc896fa",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Preprocessing Steps Completed:\n",
    "1. ✅ Handled missing values\n",
    "2. ✅ Treated outliers\n",
    "3. ✅ Engineered new features\n",
    "4. ✅ Encoded categorical variables\n",
    "5. ✅ Scaled numerical features\n",
    "6. ✅ Split data (70-30 stratified)\n",
    "7. ✅ Saved processed datasets\n",
    "\n",
    "### Ready for Modeling!\n",
    "- Train set: **X_train_scaled**, **y_train**\n",
    "- Test set: **X_test_scaled**, **y_test**\n",
    "- Feature count: [Fill in after processing]\n",
    "\n",
    "---\n",
    "**Proceed to:** `03_model_training_evaluation.ipynb`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
